{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "195vq92BEznQ9Gw1RnICwOhgUmLCFdcsC",
      "authorship_tag": "ABX9TyM4IvtfeV0OGOnwBAQLH+pu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmccentee1488/Tawhiri/blob/master/Extraction_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install pymupdf"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tb6M800h1sWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install \"camelot-py[base]\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "E8uPjEvK1kBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuqRU6TJg3gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import csv\n",
        "import pdfminer\n",
        "import camelot\n",
        "import fitz  # PyMuPDF for image extraction\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "import re # Import the 're' module for regular expressions\n",
        "\n",
        "# Define directories\n",
        "BASE_DIR = \"/content/Bio_ Mechanical PDF\"\n",
        "OUTPUT_DIR = \"/content/extracted_data\"\n",
        "METADATA_FILE = os.path.join(OUTPUT_DIR, \"processed_files.csv\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"images\"), exist_ok=True) # Create images subdirectory\n",
        "\n",
        "# CSV Paths\n",
        "TEXT_CSV = os.path.join(OUTPUT_DIR, \"extracted_text.csv\")\n",
        "TABLE_CSV = os.path.join(OUTPUT_DIR, \"extracted_tables.csv\")\n",
        "IMAGE_CSV = os.path.join(OUTPUT_DIR, \"extracted_images.csv\")\n",
        "\n",
        "# Function to scan all PDFs in subdirectories\n",
        "def get_pdf_files(directory):\n",
        "    pdf_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "# Function to check if a file has already been processed\n",
        "def is_processed(file_name):\n",
        "    if os.path.exists(METADATA_FILE):\n",
        "        processed_files = pd.read_csv(METADATA_FILE)\n",
        "        return file_name in processed_files['File_Name'].values\n",
        "    return False\n",
        "\n",
        "# Function to normalize extracted text\n",
        "def normalize_text(text):\n",
        "    text = text.replace(\"\\n\", \" \").strip()  # Remove line breaks\n",
        "    text = \" \".join(text.split())  # Normalize spacing\n",
        "    return text\n",
        "\n",
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        return normalize_text(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to extract tables from a PDF\n",
        "def extract_tables_from_pdf(file_path):\n",
        "    tables = camelot.read_pdf(file_path, pages=\"all\")\n",
        "    extracted_tables = []\n",
        "    for i, table in enumerate(tables):\n",
        "        df = table.df\n",
        "        extracted_tables.append((i+1, df.to_csv(index=False)))  # Store table with page number\n",
        "    return extracted_tables\n",
        "\n",
        "# Function to extract images from a PDF (Modified to save images)\n",
        "def extract_images_from_pdf(file_path):\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        image_records = []\n",
        "        for page_num, page in enumerate(doc):\n",
        "            for img_index, img in enumerate(page.get_images(full=True)):\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "\n",
        "                # Create a filename for the extracted image\n",
        "                image_name = f\"{os.path.splitext(os.path.basename(file_path))[0]}_page_{page_num + 1}_{img_index}.{image_ext}\"\n",
        "                image_path = os.path.join(OUTPUT_DIR, \"images\", image_name)\n",
        "\n",
        "                # Save the image\n",
        "                with open(image_path, \"wb\") as image_file:\n",
        "                    image_file.write(image_bytes)\n",
        "\n",
        "                image_records.append([os.path.basename(file_path), os.path.dirname(file_path), page_num + 1, image_name])\n",
        "        return image_records\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting images from {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "def extract_metadata(file_path):\n",
        "    \"\"\"Extracts metadata from a PDF file.\"\"\"\n",
        "    # Placeholder - Implement your metadata extraction logic here\n",
        "    # Using PyMuPDF (fitz)\n",
        "    doc = fitz.open(file_path)\n",
        "    metadata = doc.metadata\n",
        "\n",
        "    # Check for 'author' key and handle if missing\n",
        "    author = metadata.get('author', 'Unknown')\n",
        "\n",
        "    # Check for 'keywords' key and handle if missing\n",
        "    keywords = metadata.get('keywords', 'Unknown')\n",
        "\n",
        "    # Check for 'creationDate' key and handle if missing\n",
        "    creation_date = metadata.get('creationDate', None)\n",
        "    year = None\n",
        "    if creation_date:\n",
        "        match = re.search(r'D:(\\d{4})', creation_date)  # Search for year pattern\n",
        "        if match:\n",
        "            year = match.group(1)\n",
        "\n",
        "    return {\n",
        "        'File_Name': os.path.basename(file_path),\n",
        "        'Folder_Name': os.path.dirname(file_path),\n",
        "        'Author': author, # Provide default value if missing\n",
        "        'Year': year, # Provide default value if missing\n",
        "        'Keywords': keywords # Provide default value if missing\n",
        "    }\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "\n",
        "\n",
        "# Main loop for PDF extraction\n",
        "pdf_files = get_pdf_files(BASE_DIR)\n",
        "\n",
        "data_records = []\n",
        "table_records = []\n",
        "image_records = []\n",
        "\n",
        "for file_path in pdf_files:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    folder_name = os.path.dirname(file_path)\n",
        "\n",
        "    if is_processed(file_name):\n",
        "        print(f\"Skipping {file_name}, already processed.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {file_name}...\")\n",
        "\n",
        "    # Extract text and tables\n",
        "    text_content = extract_text_from_pdf(file_path)\n",
        "    tables = extract_tables_from_pdf(file_path)\n",
        "    images = extract_images_from_pdf(file_path)\n",
        "    metadata = extract_metadata(file_path)\n",
        "\n",
        "    # Save extracted text as structured data\n",
        "    data_records.append([\n",
        "        metadata['File_Name'],\n",
        "        metadata['Folder_Name'],\n",
        "        metadata['Folder_Name'],  # Category (using folder name for now)\n",
        "        text_content,\n",
        "        metadata['Author'],\n",
        "        metadata['Year'],\n",
        "        len(images), # Image count\n",
        "        len(tables), # Table count\n",
        "        metadata['Keywords'] # Keywords\n",
        "    ])\n",
        "\n",
        "    # Save extracted tables separately\n",
        "    for page_num, table_content in tables:\n",
        "        table_records.append([file_name, page_num, table_content])\n",
        "\n",
        "    # Save extracted images separately\n",
        "    image_records.extend(images)\n",
        "\n",
        "# Save text-based extractions to CSV\n",
        "with open(TEXT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"File_Name\", \"Folder_Name\", \"Category\", \"Normalized Text\", \"Author\", \"Year\", \"Image_Count\", \"Table_Count\", \"Keywords\"])\n",
        "    writer.writerows(data_records)\n",
        "\n",
        "# Save table-based extractions to CSV\n",
        "with open(TABLE_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"File_Name\", \"Page Number\", \"Table Content\"])\n",
        "    writer.writerows(table_records)\n",
        "\n",
        "# Save image-based extractions to CSV\n",
        "with open(IMAGE_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"File_Name\", \"Category\", \"Page Number\", \"Image_Name\"]) # Include image name\n",
        "    writer.writerows(image_records)\n",
        "\n",
        "# Update the list of processed files\n",
        "with open(METADATA_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"File_Name\"])\n",
        "    for record in data_records:\n",
        "        writer.writerow([record[0]])\n",
        "\n",
        "print(\"Extraction complete. Data saved to CSV.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZraELNa6BiSL",
        "outputId": "45a93a1d-8e06-4f09-9bc8-f09e020c1d22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Muscle_activity_during_the_golf_swing.pdf...\n",
            "Processing assafspu,+Journal+manager,+1262-3672-1-CE.pdf...\n",
            "Processing SAJSM+37+Masoudi+final+2.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/camelot/utils.py:1217: UserWarning:   (41.04, 43.8) does not lie in column range (44.57101933924254, 562.3266962127316)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing SAJSM+36+Radulovic+final.pdf...\n",
            "Processing assafspu,+Journal+manager,+216_sasma_2006_18_3_45779_80_91.pdf...\n",
            "Processing assafspu,+Journal+manager,+527-914-1-CE.pdf...\n",
            "Extraction complete. Data saved to CSV.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pdfminer.six"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aKbTQPSL1cf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "pdf_files = get_pdf_files(BASE_DIR)\n",
        "print(pdf_files)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodGKzXM1OKS",
        "outputId": "35cd62e3-9fcc-4ee4-8e0c-4b1161ef9add"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_PpdKa3nIRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xkp0OL6GnD6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document: Comprehensive Guide to PDF Extraction Script\n",
        "\n",
        "Overview\n",
        "\n",
        "This document provides a detailed explanation of the PDF extraction script, which processes PDF files located in /Users/johnmcentee/Project_Golfusion/PDF_Directory and its subdirectories. The script extracts text, tables, and images, organizing the data into structured CSV files for further analysis.\n",
        "\n",
        "Functionalities of the Script\n",
        "\n",
        "1. Recursive Directory Scanning\n",
        "\n",
        "The script scans all subdirectories within /Users/johnmcentee/Project_Golfusion/PDF_Directory to find all available PDF files.\n",
        "\n",
        "Uses the os.walk() function to recursively iterate over all folders.\n",
        "\n",
        "Collects paths to PDF files for processing.\n",
        "\n",
        "2. Deduplication of Processed Files\n",
        "\n",
        "A metadata file (processed_files.csv) is maintained to track processed files.\n",
        "\n",
        "If a file has already been processed, it is skipped to avoid duplicate work.\n",
        "\n",
        "Uses pandas to check existing records before proceeding.\n",
        "\n",
        "3. Text Extraction\n",
        "\n",
        "Utilizes pdfminer.six to extract textual content from PDFs.\n",
        "\n",
        "Extracted text is normalized by:\n",
        "\n",
        "Removing excessive whitespace.\n",
        "\n",
        "Eliminating unnecessary line breaks.\n",
        "\n",
        "Converting text to a uniform format.\n",
        "\n",
        "The extracted text is stored in extracted_text.csv.\n",
        "\n",
        "CSV Structure for Extracted Text (extracted_text.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Folder_Name\n",
        "\n",
        "Category\n",
        "\n",
        "Normalized Text\n",
        "\n",
        "Author\n",
        "\n",
        "Year Published\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "Folder1\n",
        "\n",
        "Folder1\n",
        "\n",
        "Golf swing ...\n",
        "\n",
        "Unknown\n",
        "\n",
        "Unknown\n",
        "\n",
        "4. Table Extraction\n",
        "\n",
        "Uses Camelot to detect and extract tabular data from PDFs.\n",
        "\n",
        "Extracts tables from all pages and converts them into structured CSV format.\n",
        "\n",
        "Tables are stored in extracted_tables.csv.\n",
        "\n",
        "CSV Structure for Extracted Tables (extracted_tables.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Page Number\n",
        "\n",
        "Table Content\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "2\n",
        "\n",
        "\"Column1, Column2 ... \"\n",
        "\n",
        "5. Image Extraction\n",
        "\n",
        "Uses PyMuPDF (fitz) to extract images embedded in PDFs.\n",
        "\n",
        "Identifies all images and logs their file name, category (folder name), and page number.\n",
        "\n",
        "Metadata is stored in extracted_images.csv.\n",
        "\n",
        "CSV Structure for Extracted Images (extracted_images.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Category\n",
        "\n",
        "Page Number\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "Folder1\n",
        "\n",
        "2\n",
        "\n",
        "6. Storing Extracted Data\n",
        "\n",
        "Three structured CSV files are generated in the extracted_data/ directory:\n",
        "\n",
        "extracted_text.csv (contains extracted text content)\n",
        "\n",
        "extracted_tables.csv (contains extracted tabular data)\n",
        "\n",
        "extracted_images.csv (contains metadata of extracted images)\n",
        "\n",
        "A metadata file (processed_files.csv) maintains a record of processed files to prevent reprocessing.\n",
        "\n",
        "Technologies and Libraries Used\n",
        "\n",
        "os: For file and directory operations.\n",
        "\n",
        "csv: For handling CSV file writing.\n",
        "\n",
        "pandas: For efficient data management and deduplication.\n",
        "\n",
        "pdfminer.six: For extracting text from PDF files.\n",
        "\n",
        "camelot: For extracting tables from PDF files.\n",
        "\n",
        "PyMuPDF (fitz): For extracting images from PDF files.\n",
        "\n",
        "Execution Process\n",
        "\n",
        "Locate all PDFs in /Users/johnmcentee/Project_Golfusion/PDF_Directory and subfolders.\n",
        "\n",
        "Check if each file has been processed using processed_files.csv.\n",
        "\n",
        "Extract and normalize text, then save to extracted_text.csv.\n",
        "\n",
        "Extract tables, then save to extracted_tables.csv.\n",
        "\n",
        "Extract images, then save metadata to extracted_images.csv.\n",
        "\n",
        "Update processed_files.csv with newly processed files.\n",
        "\n",
        "Enhancements & Next Steps\n",
        "\n",
        "Integrate NLP: Implement Named Entity Recognition (NER) or Semantic Search to analyze extracted text.\n",
        "\n",
        "Database Storage: Instead of CSV, store extracted data in BigQuery or SQLite for better querying.\n",
        "\n",
        "Image Processing: Extract and store actual image files for advanced analysis.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "This script provides an automated and efficient pipeline for extracting structured data from PDFs, making it easier to analyze and integrate into machine learning models or business intelligence tools. The modular design allows future enhancements such as AI-based insights, database integration, and real-time querying."
      ],
      "metadata": {
        "id": "TJASRR5vkR-J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT2Tc2KagKFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from PIL import Image, ImageEnhance\n",
        "import cv2\n",
        "import nltk\n",
        "from io import StringIO\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Directory where extracted CSVs are stored\n",
        "OUTPUT_DIR = \"/content/extracted_data\" # Update this to your CSV directory\n",
        "\n",
        "# Define EXTRACTION_DIR here, likely to be the same as OUTPUT_DIR\n",
        "EXTRACTION_DIR = OUTPUT_DIR  # Or set to another path if needed\n",
        "\n",
        "PROCESSED_DIR = os.path.join(EXTRACTION_DIR, \"processed_data\")\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "\n",
        "# File paths\n",
        "TEXT_CSV = os.path.join(EXTRACTION_DIR, \"extracted_text.csv\")\n",
        "TABLE_CSV = os.path.join(EXTRACTION_DIR, \"extracted_tables.csv\")\n",
        "IMAGE_CSV = os.path.join(EXTRACTION_DIR, \"extracted_images.csv\")\n",
        "\n",
        "# Processed file paths\n",
        "PROCESSED_TEXT_CSV = os.path.join(PROCESSED_DIR, \"processed_text.csv\")\n",
        "PROCESSED_TABLE_CSV = os.path.join(PROCESSED_DIR, \"processed_tables.csv\")\n",
        "PROCESSED_IMAGE_DIR = os.path.join(PROCESSED_DIR, \"processed_images\")\n",
        "os.makedirs(PROCESSED_IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. Text Cleaning ---\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text data.\"\"\"\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "def preprocess_text():\n",
        "    \"\"\"Clean and preprocess extracted text.\"\"\"\n",
        "    print(\"Processing text data...\")\n",
        "    df = pd.read_csv(TEXT_CSV)\n",
        "    df['Normalized Text'] = df['Normalized Text'].apply(clean_text)\n",
        "    df.to_csv(PROCESSED_TEXT_CSV, index=False)\n",
        "    print(f\"Processed text saved to {PROCESSED_TEXT_CSV}\")\n",
        "\n",
        "# --- 2. Table Cleaning ---\n",
        "def clean_table(dataframe):\n",
        "    \"\"\"Clean and normalize table data.\"\"\"\n",
        "    # Fill missing values with 'Unknown'\n",
        "    dataframe.fillna('Unknown', inplace=True)\n",
        "    # Example: Normalize weights if a 'Weight' column exists\n",
        "    if 'Weight' in dataframe.columns:\n",
        "        dataframe['Weight'] = dataframe['Weight'].apply(\n",
        "            lambda x: x * 0.453592 if 'lbs' in str(x) else x\n",
        "        )\n",
        "    return dataframe\n",
        "\n",
        "def preprocess_tables():\n",
        "    \"\"\"Clean and preprocess extracted table data.\"\"\"\n",
        "    print(\"Processing table data...\")\n",
        "    df = pd.read_csv(TABLE_CSV)\n",
        "    # Assumes the table content is stored as strings in one column\n",
        "    df['Cleaned Table Content'] = df['Table Content'].apply(\n",
        "        lambda x: clean_table(pd.read_csv(StringIO(x))).to_csv(index=False)\n",
        "    )\n",
        "    df.to_csv(PROCESSED_TABLE_CSV, index=False)\n",
        "    print(f\"Processed tables saved to {PROCESSED_TABLE_CSV}\")\n",
        "\n",
        "# --- 3. Image Preprocessing ---\n",
        "def preprocess_images():\n",
        "    \"\"\"Preprocess images and save them to the processed directory.\"\"\"\n",
        "    print(\"Processing images...\")\n",
        "    df = pd.read_csv(IMAGE_CSV)\n",
        "\n",
        "    # Assuming the image name is the last column in your CSV\n",
        "    image_name_column = df.columns[-1]  # Get the name of the last column\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        file_name = row['File_Name']\n",
        "        category = row['Category']\n",
        "        page_number = row['Page Number']\n",
        "        image_name = row[image_name_column] # Get the extracted image filename using the dynamic column name\n",
        "\n",
        "        # Construct the input image path (using the extracted image filename)\n",
        "        input_image_path = os.path.join(EXTRACTION_DIR, \"images\", image_name)\n",
        "\n",
        "        # Construct the output image path\n",
        "        output_image_path = os.path.join(PROCESSED_IMAGE_DIR, f\"{os.path.splitext(file_name)[0]}_page_{page_number}.jpg\")\n",
        "\n",
        "        # Assuming you have a preprocess_image function\n",
        "        #preprocess_image(input_image_path, output_image_path)\n",
        "\n",
        "        print(f\"Processed images saved to {PROCESSED_IMAGE_DIR}\")\n",
        "\n",
        "# --- Main Pipeline Execution ---\n",
        "def main():\n",
        "    \"\"\"Run the full preprocessing pipeline.\"\"\"\n",
        "    if not os.path.exists(TEXT_CSV):\n",
        "        print(f\"Text CSV not found at {TEXT_CSV}. Skipping text processing.\")\n",
        "    else:\n",
        "        preprocess_text()\n",
        "\n",
        "    if not os.path.exists(TABLE_CSV):\n",
        "        print(f\"Table CSV not found at {TABLE_CSV}. Skipping table processing.\")\n",
        "    else:\n",
        "        preprocess_tables()\n",
        "\n",
        "    if not os.path.exists(IMAGE_CSV):\n",
        "        print(f\"Image CSV not found at {IMAGE_CSV}. Skipping image processing.\")\n",
        "    else:\n",
        "        preprocess_images()\n",
        "\n",
        "    print(\"Preprocessing complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-2F3xP4gMLK",
        "outputId": "1c578941-7d42-4f94-9152-aa6d6eb5f97c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text data...\n",
            "Processed text saved to /content/extracted_data/processed_data/processed_text.csv\n",
            "Processing table data...\n",
            "Processed tables saved to /content/extracted_data/processed_data/processed_tables.csv\n",
            "Processing images...\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Processed images saved to /content/extracted_data/processed_data/processed_images\n",
            "Preprocessing complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n",
            "<ipython-input-12-0fbe2b6349f3>:58: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  dataframe.fillna('Unknown', inplace=True)\n"
          ]
        }
      ]
    }
  ]
}