{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "195vq92BEznQ9Gw1RnICwOhgUmLCFdcsC",
      "authorship_tag": "ABX9TyMGEFcRNTi+x5GBwKWiN+xX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmccentee1488/Tawhiri/blob/master/Extraction_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ybyqcgI59N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import csv\n",
        "import pdfminer\n",
        "import camelot\n",
        "import fitz  # PyMuPDF for image extraction\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "\n",
        "\n",
        "# Define directories\n",
        "BASE_DIR = \"/content/Bio Mechanical PDF\"\n",
        "OUTPUT_DIR = \"/content/extracted_data\"\n",
        "METADATA_FILE = os.path.join(OUTPUT_DIR, \"processed_files.csv\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# CSV Paths\n",
        "TEXT_CSV = os.path.join(OUTPUT_DIR, \"extracted_text.csv\")\n",
        "TABLE_CSV = os.path.join(OUTPUT_DIR, \"extracted_tables.csv\")\n",
        "IMAGE_CSV = os.path.join(OUTPUT_DIR, \"extracted_images.csv\")\n",
        "\n",
        "# Function to scan all PDFs in subdirectories\n",
        "def get_pdf_files(directory):\n",
        "    pdf_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "# Function to check if a file has already been processed\n",
        "def is_processed(file_name):\n",
        "    if os.path.exists(METADATA_FILE):\n",
        "        try:\n",
        "            processed_files = pd.read_csv(METADATA_FILE)\n",
        "            return file_name in processed_files['File_Name'].values\n",
        "        except pd.errors.EmptyDataError:  # Handle empty CSV\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "# Function to normalize extracted text\n",
        "def normalize_text(text):\n",
        "    text = text.replace(\"\\n\", \" \").strip()  # Remove line breaks\n",
        "    text = \" \".join(text.split())  # Normalize spacing\n",
        "    return text\n",
        "\n",
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        return normalize_text(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to extract tables from a PDF\n",
        "def extract_tables_from_pdf(file_path):\n",
        "    tables = camelot.read_pdf(file_path, pages=\"all\")\n",
        "    extracted_tables = []\n",
        "    for i, table in enumerate(tables):\n",
        "        df = table.df\n",
        "        extracted_tables.append((i+1, df.to_csv(index=False)))  # Store table with page number\n",
        "    return extracted_tables\n",
        "\n",
        "# Function to extract images from a PDF\n",
        "def extract_images_from_pdf(file_path):\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        image_records = []\n",
        "        for page_num, page in enumerate(doc):\n",
        "            for img_index, img in enumerate(page.get_images(full=True)):\n",
        "                image_records.append([os.path.basename(file_path), os.path.dirname(file_path), page_num + 1])\n",
        "        return image_records\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting images from {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_metadata(file_path):\n",
        "    \"\"\"Extracts metadata from a PDF file.\"\"\"\n",
        "    metadata = {}\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            parser = PDFParser(f)\n",
        "            document = PDFDocument(parser)\n",
        "\n",
        "            metadata['File_Name'] = os.path.basename(file_path)\n",
        "            metadata['Folder_Name'] = os.path.dirname(file_path)\n",
        "\n",
        "            # Check if document.info is a dictionary\n",
        "            if isinstance(document.info, dict):\n",
        "                metadata['Keywords'] = document.info.get('Keywords', 'Unknown')\n",
        "                metadata['Author'] = document.info.get('Author', 'Unknown')\n",
        "                metadata['Year'] = document.info.get('CreationDate', 'Unknown')\n",
        "\n",
        "                if metadata['Year'] and metadata['Year'] != 'Unknown':\n",
        "                    metadata['Year'] = metadata['Year'][:4]  # Extract year\n",
        "                else:\n",
        "                    metadata['Year'] = 'Unknown'\n",
        "            else:\n",
        "                # Handle the case where document.info is not a dictionary (e.g., a list)\n",
        "                metadata['Keywords'] = 'Unknown'\n",
        "                metadata['Author'] = 'Unknown'\n",
        "                metadata['Year'] = 'Unknown'\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting metadata from {file_path}: {e}\")\n",
        "        metadata['Author'] = 'Unknown'\n",
        "        metadata['Year'] = 'Unknown'\n",
        "        metadata['Keywords'] = 'Unknown' # Added to handle Keywords in case of error\n",
        "\n",
        "    return metadata\n",
        "\n",
        "pdf_files = get_pdf_files(BASE_DIR)\n",
        "\n",
        "data_records = []\n",
        "table_records = []\n",
        "image_records = []\n",
        "\n",
        "for file_path in pdf_files:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    folder_name = os.path.dirname(file_path)\n",
        "\n",
        "    if is_processed(file_name):\n",
        "        print(f\"Skipping {file_name}, already processed.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {file_name}...\")\n",
        "\n",
        "    # Extract text and tables\n",
        "    text_content = extract_text_from_pdf(file_path)\n",
        "    tables = extract_tables_from_pdf(file_path)\n",
        "    images = extract_images_from_pdf(file_path)\n",
        "\n",
        "    # Extract metadata\n",
        "    metadata = extract_metadata(file_path)\n",
        "\n",
        "\n",
        "    # Get image and table counts\n",
        "    image_count = len(images)\n",
        "    table_count = len(tables)\n",
        "\n",
        "    # Save extracted text as structured data\n",
        "    data_records.append([\n",
        "        metadata['File_Name'],\n",
        "        metadata['Folder_Name'],\n",
        "        metadata['Folder_Name'],  # Category (using folder name for now)\n",
        "        text_content,\n",
        "        metadata['Author'],\n",
        "        metadata['Year'],\n",
        "        image_count, # Added image count\n",
        "        table_count, # Added table count\n",
        "        metadata.get('Keywords', 'Unknown') # Added keywords, use get to handle missing keys\n",
        "    ])\n",
        "\n",
        "    # Save extracted tables separately\n",
        "    for page_num, table_content in tables:\n",
        "        table_records.append([file_name, page_num, table_content])\n",
        "\n",
        "    # Save extracted images separately\n",
        "    image_records.extend(images)\n",
        "\n",
        "# Save text-based extractions to CSV\n",
        "# 'a' mode for appending\n",
        "with open(TEXT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # Write header only if file is empty or doesn't exist\n",
        "    if not os.path.exists(TEXT_CSV) or os.stat(TEXT_CSV).st_size == 0:\n",
        "        writer.writerow([\"File_Name\", \"Folder_Name\", \"Category\", \"Normalized Text\", \"Author\", \"Year\", \"Image_Count\", \"Table_Count\", \"Keywords\"])\n",
        "    writer.writerows(data_records)\n",
        "\n",
        "# Save table-based extractions to CSV\n",
        "# 'a' mode for appending\n",
        "with open(TABLE_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # Write header only if file is empty or doesn't exist\n",
        "    if not os.path.exists(TABLE_CSV) or os.stat(TABLE_CSV).st_size == 0:\n",
        "        writer.writerow([\"File_Name\", \"Page Number\", \"Table Content\"])\n",
        "    writer.writerows(table_records)\n",
        "\n",
        "# Save image-based extractions to CSV\n",
        "# 'a' mode for appending\n",
        "with open(IMAGE_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # Write header only if file is empty or doesn't exist\n",
        "    if not os.path.exists(IMAGE_CSV) or os.stat(IMAGE_CSV).st_size == 0:\n",
        "        writer.writerow([\"File_Name\", \"Category\", \"Page Number\"])\n",
        "    writer.writerows(image_records)\n",
        "\n",
        "# Update the list of processed files\n",
        "# 'a' mode for appending\n",
        "with open(METADATA_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    # Write header only if file is empty or doesn't exist\n",
        "    if not os.path.exists(METADATA_FILE) or os.stat(METADATA_FILE).st_size == 0:\n",
        "        writer.writerow([\"File_Name\"])\n",
        "    for record in data_records:\n",
        "        writer.writerow([record[0]])\n",
        "\n",
        "print(\"Extraction complete. Data saved to CSV.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPYwBKtJAC70",
        "outputId": "0e86d9ed-ed5f-47a9-d012-c1679337c893"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Muscle_activity_during_the_golf_swing.pdf, already processed.\n",
            "Processing Enhance_Information_Propagation_for_Grap.pdf...\n",
            "Skipping assafspu,+Journal+manager,+1262-3672-1-CE.pdf, already processed.\n",
            "Skipping Three_Dimensional_Kinematics_Observed_Be.pdf, already processed.\n",
            "Skipping A_Comprehensive_Survey_on_Graph_Neural_N.pdf, already processed.\n",
            "Skipping SAJSM+37+Masoudi+final+2.pdf, already processed.\n",
            "Skipping SAJSM+36+Radulovic+final.pdf, already processed.\n",
            "Processing Estimating_Upper_Extremity_Joint_Contrib.pdf...\n",
            "Processing Convolutional_Graph_Neural_Networks.pdf...\n",
            "Extraction complete. Data saved to CSV.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pymupdf"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tb6M800h1sWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install \"camelot-py[base]\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "E8uPjEvK1kBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install pdfminer.six"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aKbTQPSL1cf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "pdf_files = get_pdf_files(BASE_DIR)\n",
        "print(pdf_files)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodGKzXM1OKS",
        "outputId": "30e0a547-eed8-4f6e-dec2-dd8bf24971e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/Bio Mechanical PDF/Muscle_activity_during_the_golf_swing.pdf', '/content/Bio Mechanical PDF/assafspu,+Journal+manager,+1262-3672-1-CE.pdf', '/content/Bio Mechanical PDF/Three_Dimensional_Kinematics_Observed_Be.pdf', '/content/Bio Mechanical PDF/SAJSM+37+Masoudi+final+2.pdf', '/content/Bio Mechanical PDF/SAJSM+36+Radulovic+final.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_PpdKa3nIRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xkp0OL6GnD6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document: Comprehensive Guide to PDF Extraction Script\n",
        "\n",
        "Overview\n",
        "\n",
        "This document provides a detailed explanation of the PDF extraction script, which processes PDF files located in /Users/johnmcentee/Project_Golfusion/PDF_Directory and its subdirectories. The script extracts text, tables, and images, organizing the data into structured CSV files for further analysis.\n",
        "\n",
        "Functionalities of the Script\n",
        "\n",
        "1. Recursive Directory Scanning\n",
        "\n",
        "The script scans all subdirectories within /Users/johnmcentee/Project_Golfusion/PDF_Directory to find all available PDF files.\n",
        "\n",
        "Uses the os.walk() function to recursively iterate over all folders.\n",
        "\n",
        "Collects paths to PDF files for processing.\n",
        "\n",
        "2. Deduplication of Processed Files\n",
        "\n",
        "A metadata file (processed_files.csv) is maintained to track processed files.\n",
        "\n",
        "If a file has already been processed, it is skipped to avoid duplicate work.\n",
        "\n",
        "Uses pandas to check existing records before proceeding.\n",
        "\n",
        "3. Text Extraction\n",
        "\n",
        "Utilizes pdfminer.six to extract textual content from PDFs.\n",
        "\n",
        "Extracted text is normalized by:\n",
        "\n",
        "Removing excessive whitespace.\n",
        "\n",
        "Eliminating unnecessary line breaks.\n",
        "\n",
        "Converting text to a uniform format.\n",
        "\n",
        "The extracted text is stored in extracted_text.csv.\n",
        "\n",
        "CSV Structure for Extracted Text (extracted_text.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Folder_Name\n",
        "\n",
        "Category\n",
        "\n",
        "Normalized Text\n",
        "\n",
        "Author\n",
        "\n",
        "Year Published\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "Folder1\n",
        "\n",
        "Folder1\n",
        "\n",
        "Golf swing ...\n",
        "\n",
        "Unknown\n",
        "\n",
        "Unknown\n",
        "\n",
        "4. Table Extraction\n",
        "\n",
        "Uses Camelot to detect and extract tabular data from PDFs.\n",
        "\n",
        "Extracts tables from all pages and converts them into structured CSV format.\n",
        "\n",
        "Tables are stored in extracted_tables.csv.\n",
        "\n",
        "CSV Structure for Extracted Tables (extracted_tables.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Page Number\n",
        "\n",
        "Table Content\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "2\n",
        "\n",
        "\"Column1, Column2 ... \"\n",
        "\n",
        "5. Image Extraction\n",
        "\n",
        "Uses PyMuPDF (fitz) to extract images embedded in PDFs.\n",
        "\n",
        "Identifies all images and logs their file name, category (folder name), and page number.\n",
        "\n",
        "Metadata is stored in extracted_images.csv.\n",
        "\n",
        "CSV Structure for Extracted Images (extracted_images.csv)\n",
        "\n",
        "File_Name\n",
        "\n",
        "Category\n",
        "\n",
        "Page Number\n",
        "\n",
        "sample.pdf\n",
        "\n",
        "Folder1\n",
        "\n",
        "2\n",
        "\n",
        "6. Storing Extracted Data\n",
        "\n",
        "Three structured CSV files are generated in the extracted_data/ directory:\n",
        "\n",
        "extracted_text.csv (contains extracted text content)\n",
        "\n",
        "extracted_tables.csv (contains extracted tabular data)\n",
        "\n",
        "extracted_images.csv (contains metadata of extracted images)\n",
        "\n",
        "A metadata file (processed_files.csv) maintains a record of processed files to prevent reprocessing.\n",
        "\n",
        "Technologies and Libraries Used\n",
        "\n",
        "os: For file and directory operations.\n",
        "\n",
        "csv: For handling CSV file writing.\n",
        "\n",
        "pandas: For efficient data management and deduplication.\n",
        "\n",
        "pdfminer.six: For extracting text from PDF files.\n",
        "\n",
        "camelot: For extracting tables from PDF files.\n",
        "\n",
        "PyMuPDF (fitz): For extracting images from PDF files.\n",
        "\n",
        "Execution Process\n",
        "\n",
        "Locate all PDFs in /Users/johnmcentee/Project_Golfusion/PDF_Directory and subfolders.\n",
        "\n",
        "Check if each file has been processed using processed_files.csv.\n",
        "\n",
        "Extract and normalize text, then save to extracted_text.csv.\n",
        "\n",
        "Extract tables, then save to extracted_tables.csv.\n",
        "\n",
        "Extract images, then save metadata to extracted_images.csv.\n",
        "\n",
        "Update processed_files.csv with newly processed files.\n",
        "\n",
        "Enhancements & Next Steps\n",
        "\n",
        "Integrate NLP: Implement Named Entity Recognition (NER) or Semantic Search to analyze extracted text.\n",
        "\n",
        "Database Storage: Instead of CSV, store extracted data in BigQuery or SQLite for better querying.\n",
        "\n",
        "Image Processing: Extract and store actual image files for advanced analysis.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "This script provides an automated and efficient pipeline for extracting structured data from PDFs, making it easier to analyze and integrate into machine learning models or business intelligence tools. The modular design allows future enhancements such as AI-based insights, database integration, and real-time querying."
      ],
      "metadata": {
        "id": "TJASRR5vkR-J"
      }
    }
  ]
}